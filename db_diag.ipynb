{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c49120-f742-4dc9-b155-41f6f69c9422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_r_7i.db\n",
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_Qgauss_2.db\n",
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_Qcauchy_1.db\n",
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_QLevyII_1.db\n",
      "\n",
      "‚úÖ Sampling diagnostics complete across 4 databases.\n",
      "üìÑ Results saved to: s2_sampling_diagnostics_merged.csv\n",
      "   Parameter 1 Parameter 2  Populated Bins  Total Bins   Entropy  Gini Index  \\\n",
      "22        rhos          cs              25          25  3.100838    0.950664   \n",
      "43          cs          H2              25          25  3.069214    0.947349   \n",
      "28        rhos          H2              25          25  3.022123    0.943577   \n",
      "49        phis          H2              20          20  2.801119    0.928010   \n",
      "24        rhos        epss              20          20  2.776625    0.926216   \n",
      "25        rhos     thetass              20          20  2.766834    0.924574   \n",
      "26        rhos         hks              20          20  2.774837    0.925718   \n",
      "27        rhos          H1              20          20  2.791045    0.927416   \n",
      "29        rhos          x6              20          20  2.836083    0.933570   \n",
      "30         nus          cs              20          20  2.835536    0.932260   \n",
      "\n",
      "    Avg Samples/Bin  Min Samples/Bin  PPI Grid Rank  Sufficient  \n",
      "22           190.28               47              5        True  \n",
      "43           190.28               50              5        True  \n",
      "28           190.28               33              5        True  \n",
      "49           237.85               44              4        True  \n",
      "24           237.85               34              4        True  \n",
      "25           237.85               35              4        True  \n",
      "26           237.85               32              4        True  \n",
      "27           237.85               34              4        True  \n",
      "29           237.85               28              4        True  \n",
      "30           237.85               90              4        True  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import matrix_rank\n",
    "import os\n",
    "\n",
    "# === USER CONFIGURATION ===\n",
    "DB_PATHS = [\n",
    "    \"/Users/......./pso_r_7i.db\",\n",
    "    \"/Users/......./pso_Qgauss_2.db\",\n",
    "    \"/Users/......./pso_Qcauchy_1.db\",\n",
    "    \"/Users/......./pso_QLevyII_1.db\"\n",
    "    # Add more paths here if needed\n",
    "]\n",
    "PARAM_COLS = [\"loadb\", \"Es\", \"rhos\", \"nus\", \"cs\", \"phis\",\n",
    "              \"epss\", \"thetass\", \"hks\", \"H1\", \"H2\", \"x6\"]\n",
    "MIN_BIN_THRESHOLD = 15\n",
    "N_BINS = 5\n",
    "OUTPUT_CSV = \"s2_sampling_diagnostics_merged.csv\"\n",
    "\n",
    "# === MERGE DATA FROM MULTIPLE DATABASES ===\n",
    "all_data = []\n",
    "for path in DB_PATHS:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ö†Ô∏è Skipping missing file: {path}\")\n",
    "        continue\n",
    "    print(f\"‚úÖ Loading: {path}\")\n",
    "    conn = sqlite3.connect(path)\n",
    "    query = f\"SELECT {', '.join(PARAM_COLS)}, e_plst FROM particles\"\n",
    "    try:\n",
    "        df_part = pd.read_sql_query(query, conn)\n",
    "        all_data.append(df_part)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to read from {path}: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Merge all data\n",
    "if not all_data:\n",
    "    raise ValueError(\"‚ùå No valid data loaded from the listed databases.\")\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# === CLEAN DATA ===\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(subset=[\"e_plst\"], inplace=True)\n",
    "\n",
    "# === SAMPLING DIAGNOSTICS ===\n",
    "sufficiency_diagnostics = []\n",
    "\n",
    "for i, j in combinations(PARAM_COLS, 2):\n",
    "    try:\n",
    "        df[\"bin_i\"] = pd.qcut(df[i], q=N_BINS, duplicates=\"drop\")\n",
    "        df[\"bin_j\"] = pd.qcut(df[j], q=N_BINS, duplicates=\"drop\")\n",
    "\n",
    "        bin_counts = df.groupby([\"bin_i\", \"bin_j\"], observed=True).size().unstack(fill_value=0)\n",
    "        mean_ppi_grid = df.groupby([\"bin_i\", \"bin_j\"], observed=True)[\"e_plst\"].mean().unstack(fill_value=np.nan)\n",
    "\n",
    "        count_values = bin_counts.values.flatten()\n",
    "        p = count_values[count_values > 0] / count_values.sum()\n",
    "\n",
    "        populated_bins = (count_values > 0).sum()\n",
    "        total_bins = bin_counts.size\n",
    "        bin_entropy = entropy(p)\n",
    "        bin_gini = 1 - np.sum(p ** 2)\n",
    "        avg_samples = np.mean(count_values[count_values > 0])\n",
    "        min_samples = np.min(count_values[count_values > 0])\n",
    "        rank = matrix_rank(mean_ppi_grid.fillna(mean_ppi_grid.mean()).values)\n",
    "\n",
    "        sufficiency_diagnostics.append({\n",
    "            \"Parameter 1\": i,\n",
    "            \"Parameter 2\": j,\n",
    "            \"Populated Bins\": populated_bins,\n",
    "            \"Total Bins\": total_bins,\n",
    "            \"Entropy\": bin_entropy,\n",
    "            \"Gini Index\": bin_gini,\n",
    "            \"Avg Samples/Bin\": avg_samples,\n",
    "            \"Min Samples/Bin\": min_samples,\n",
    "            \"PPI Grid Rank\": rank,\n",
    "            \"Sufficient\": populated_bins >= MIN_BIN_THRESHOLD\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {i}-{j}: {e}\")\n",
    "        continue\n",
    "\n",
    "# === OUTPUT ===\n",
    "df_out = pd.DataFrame(sufficiency_diagnostics)\n",
    "df_out.sort_values(by=\"Populated Bins\", ascending=False, inplace=True)\n",
    "df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Sampling diagnostics complete across {len(DB_PATHS)} databases.\")\n",
    "print(f\"üìÑ Results saved to: {OUTPUT_CSV}\")\n",
    "print(df_out.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d347fbc-08c2-442d-9ea9-5b9b4ddfb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_r_7_iter1_200.db\n",
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_Qgauss_2.db\n",
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_Qcauchy_1.db\n",
      "‚úÖ Loading: /Users/israr/Documents/2025/LSwarm/pso_QLevyII_1.db\n",
      "üìä Calculating global sampling diagnostics...\n",
      "‚úÖ Global Stats:\n",
      " - Total Samples: 6034\n",
      " - Unique Samples: 6012\n",
      " - Redundancy Ratio: 0.0036\n",
      " - Avg Pairwise Distance: 63138312.4833\n",
      "üìä Running S2 bin coverage diagnostics...\n",
      "\n",
      "=== GLOBAL STATUS EVALUATION ===\n",
      "Total Samples Status: ‚úÖ Good\n",
      "Unique Samples Status: ‚úÖ High\n",
      "Redundancy Status: ‚úÖ Low\n",
      "Pairwise Distance Status: ‚ÑπÔ∏è Relative ‚Äî compare across runs\n",
      "loadb Coverage: ‚úÖ Excellent\n",
      "Es Coverage: ‚úÖ Excellent\n",
      "rhos Coverage: ‚úÖ Excellent\n",
      "nus Coverage: ‚úÖ Excellent\n",
      "cs Coverage: ‚úÖ Excellent\n",
      "phis Coverage: ‚úÖ Excellent\n",
      "epss Coverage: ‚úÖ Excellent\n",
      "thetass Coverage: ‚úÖ Excellent\n",
      "hks Coverage: ‚úÖ Excellent\n",
      "H1 Coverage: ‚úÖ Excellent\n",
      "H2 Coverage: ‚úÖ Excellent\n",
      "x6 Coverage: ‚úÖ Excellent\n",
      "Number of S2 Pairs (False): 0\n",
      "Number of S2 Pairs (Total): 66\n",
      "S2 Sufficiency Coverage (%): 100.0\n",
      "\n",
      "‚úÖ S2 bin diagnostics saved to s2_sampling_diagnostics_merged.csv\n",
      "\n",
      "=== GLOBAL SUMMARY ===\n",
      "Total Samples: 6034\n",
      "Unique Samples: 6012\n",
      "Redundancy: 0.0036\n",
      "Avg Pairwise Distance: 63138312.4833\n",
      "Unique values in loadb: 3854\n",
      "Unique values in Es: 4325\n",
      "Unique values in rhos: 4496\n",
      "Unique values in nus: 4470\n",
      "Unique values in cs: 5491\n",
      "Unique values in phis: 3753\n",
      "Unique values in epss: 3813\n",
      "Unique values in thetass: 4003\n",
      "Unique values in hks: 3526\n",
      "Unique values in H1: 3441\n",
      "Unique values in H2: 4705\n",
      "Unique values in x6: 4456\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import entropy\n",
    "from numpy.linalg import matrix_rank\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# === USER CONFIGURATION ===\n",
    "DB_PATHS = [\n",
    "    \"/Users/......./pso_r_7_iter1_200.db\",\n",
    "    \"/Users/......./pso_Qgauss_2.db\",\n",
    "    \"/Users/......./pso_Qcauchy_1.db\",\n",
    "    \"/Users/......./pso_QLevyII_1.db\"\n",
    "    # Add more paths here if needed\n",
    "]\n",
    "# DB_PATHS = [\n",
    "#     \"/Users/israr/Documents/2025/LSwarm/pso_r_7_iter1_200.db\"\n",
    "#     # Add more paths here if needed\n",
    "# ]\n",
    "PARAM_COLS = [\"loadb\", \"Es\", \"rhos\", \"nus\", \"cs\", \"phis\",\n",
    "              \"epss\", \"thetass\", \"hks\", \"H1\", \"H2\", \"x6\"]\n",
    "MIN_BIN_THRESHOLD = 15\n",
    "N_BINS = 5\n",
    "OUTPUT_CSV = \"s2_sampling_diagnostics_merged.csv\"\n",
    "\n",
    "# === MERGE DATA FROM MULTIPLE DATABASES ===\n",
    "all_data = []\n",
    "for path in DB_PATHS:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ö†Ô∏è Skipping missing file: {path}\")\n",
    "        continue\n",
    "    print(f\"‚úÖ Loading: {path}\")\n",
    "    conn = sqlite3.connect(path)\n",
    "    query = f\"SELECT {', '.join(PARAM_COLS)}, ppi FROM particles\"\n",
    "    try:\n",
    "        df_part = pd.read_sql_query(query, conn)\n",
    "        all_data.append(df_part)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to read from {path}: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Merge all data\n",
    "if not all_data:\n",
    "    raise ValueError(\"‚ùå No valid data loaded from the listed databases.\")\n",
    "df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# === CLEAN DATA ===\n",
    "# CLEAN CORRECTLY\n",
    "df_clean = df.copy()\n",
    "df_clean.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_clean.dropna(subset=[\"ppi\"], inplace=True)\n",
    "\n",
    "# === GLOBAL DIAGNOSTICS ===\n",
    "print(\"üìä Calculating global sampling diagnostics...\")\n",
    "total_samples = len(df_clean)\n",
    "unique_samples = df_clean[PARAM_COLS].drop_duplicates().shape[0]\n",
    "redundancy_ratio = 1 - unique_samples / total_samples\n",
    "average_pairwise_distance = pdist(df_clean[PARAM_COLS].values).mean()\n",
    "unique_per_param = df_clean[PARAM_COLS].nunique().to_dict()\n",
    "\n",
    "print(\"‚úÖ Global Stats:\")\n",
    "print(f\" - Total Samples: {total_samples}\")\n",
    "print(f\" - Unique Samples: {unique_samples}\")\n",
    "print(f\" - Redundancy Ratio: {redundancy_ratio:.4f}\")\n",
    "print(f\" - Avg Pairwise Distance: {average_pairwise_distance:.4f}\")\n",
    "\n",
    "# === LOCAL DIAGNOSTICS PER PARAMETER PAIR ===\n",
    "print(\"üìä Running S2 bin coverage diagnostics...\")\n",
    "sufficiency_diagnostics = []\n",
    "\n",
    "for i, j in combinations(PARAM_COLS, 2):\n",
    "    try:\n",
    "        df_clean[\"bin_i\"] = pd.qcut(df_clean[i], q=N_BINS, duplicates=\"drop\")\n",
    "        df_clean[\"bin_j\"] = pd.qcut(df_clean[j], q=N_BINS, duplicates=\"drop\")\n",
    "\n",
    "        bin_counts = df_clean.groupby([\"bin_i\", \"bin_j\"], observed=True).size().unstack(fill_value=0)\n",
    "        mean_ppi_grid = df_clean.groupby([\"bin_i\", \"bin_j\"], observed=True)[\"ppi\"].mean().unstack(fill_value=np.nan)\n",
    "\n",
    "        count_values = bin_counts.values.flatten()\n",
    "        p = count_values[count_values > 0] / count_values.sum()\n",
    "\n",
    "        populated_bins = (count_values > 0).sum()\n",
    "        total_bins = bin_counts.size\n",
    "        bin_entropy = entropy(p)\n",
    "        bin_gini = 1 - np.sum(p ** 2)\n",
    "        avg_samples = np.mean(count_values[count_values > 0])\n",
    "        min_samples = np.min(count_values[count_values > 0])\n",
    "        rank = matrix_rank(mean_ppi_grid.fillna(mean_ppi_grid.mean()).values)\n",
    "\n",
    "        sufficiency_diagnostics.append({\n",
    "            \"Parameter 1\": i,\n",
    "            \"Parameter 2\": j,\n",
    "            \"Populated Bins\": populated_bins,\n",
    "            \"Total Bins\": total_bins,\n",
    "            \"Entropy\": bin_entropy,\n",
    "            \"Gini Index\": bin_gini,\n",
    "            \"Avg Samples/Bin\": avg_samples,\n",
    "            \"Min Samples/Bin\": min_samples,\n",
    "            \"PPI Grid Rank\": rank,\n",
    "            \"Sufficient\": populated_bins >= MIN_BIN_THRESHOLD\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipping {i}-{j}: {e}\")\n",
    "        continue\n",
    "\n",
    "# === SAVE RESULTS ===\n",
    "df_out = pd.DataFrame(sufficiency_diagnostics)\n",
    "df_out.sort_values(by=\"Populated Bins\", ascending=False, inplace=True)\n",
    "df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "num_false = (~df_out[\"Sufficient\"]).sum()\n",
    "\n",
    "# === GLOBAL STATUS EVALUATION FUNCTION ===\n",
    "def evaluate_global_quality(row):\n",
    "    status = {}\n",
    "    unique_ratio = unique_samples / total_samples\n",
    "    status[\"Total Samples Status\"] = (\n",
    "        \"‚úÖ Good\" if total_samples >= 1000 else\n",
    "        \"‚ö†Ô∏è Low\" if total_samples >= 500 else\n",
    "        \"‚ùå Very Low\"\n",
    "    )\n",
    "    status[\"Unique Samples Status\"] = (\n",
    "        \"‚úÖ High\" if unique_ratio >= 0.95 else\n",
    "        \"‚ö†Ô∏è Moderate\" if unique_ratio >= 0.9 else\n",
    "        \"‚ùå Low\"\n",
    "    )\n",
    "    status[\"Redundancy Status\"] = (\n",
    "        \"‚úÖ Low\" if redundancy_ratio <= 0.05 else\n",
    "        \"‚ö†Ô∏è Moderate\" if redundancy_ratio <= 0.15 else\n",
    "        \"‚ùå High\"\n",
    "    )\n",
    "    status[\"Pairwise Distance Status\"] = \"‚ÑπÔ∏è Relative ‚Äî compare across runs\"\n",
    "    for param in PARAM_COLS:\n",
    "        val = unique_per_param[param]\n",
    "        status[f\"{param} Coverage\"] = (\n",
    "            \"‚úÖ Excellent\" if val >= 500 else\n",
    "            \"‚ö†Ô∏è Acceptable\" if val >= 200 else\n",
    "            \"‚ùå Sparse\"\n",
    "        )\n",
    "    return status\n",
    "\n",
    "# Generate global status\n",
    "global_status = evaluate_global_quality(df)\n",
    "global_status[\"Number of S2 Pairs (False)\"] = num_false\n",
    "global_status[\"Number of S2 Pairs (Total)\"] = len(df_out)\n",
    "global_status[\"S2 Sufficiency Coverage (%)\"] = 100 * (len(df_out) - num_false) / len(df_out)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== GLOBAL STATUS EVALUATION ===\")\n",
    "for k, v in global_status.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "df_out = pd.DataFrame(sufficiency_diagnostics)\n",
    "df_out.sort_values(by=\"Populated Bins\", ascending=False, inplace=True)\n",
    "df_out.to_csv(OUTPUT_CSV, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ S2 bin diagnostics saved to {OUTPUT_CSV}\")\n",
    "\n",
    "# === Optional: Display Global Summary ===\n",
    "print(\"\\n=== GLOBAL SUMMARY ===\")\n",
    "print(f\"Total Samples: {total_samples}\")\n",
    "print(f\"Unique Samples: {unique_samples}\")\n",
    "print(f\"Redundancy: {redundancy_ratio:.4f}\")\n",
    "print(f\"Avg Pairwise Distance: {average_pairwise_distance:.4f}\")\n",
    "for k, v in unique_per_param.items():\n",
    "    print(f\"Unique values in {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404835f3-4463-4e8f-a9f6-bab5af06f3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
